{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "041932ff",
   "metadata": {},
   "source": [
    "# Model Evaluation and Interpretation - Hotel Booking Cancellation Prediction\n",
    "## Academic Research Framework - NIB 7072 Coursework\n",
    "\n",
    "**Research Objective:** Comprehensive evaluation and interpretation of trained models for hotel booking cancellation prediction.\n",
    "\n",
    "**Academic Context:** This notebook provides detailed analysis of model performance, SHAP-based interpretability, and business impact assessment for the trained models (LogReg, RandomForest, XGBoost, PyTorch MLP).\n",
    "\n",
    "**Key Areas:**\n",
    "- Model performance comparison and statistical significance testing\n",
    "- SHAP-based feature importance and model interpretability\n",
    "- Business impact analysis and revenue implications\n",
    "- Academic documentation and methodology validation\n",
    "- Sri Lankan tourism market context integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f55e179",
   "metadata": {},
   "source": [
    "## üìä Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c89a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, classification_report, confusion_matrix,\n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "# Model loading and MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import mlflow.pytorch\n",
    "import joblib\n",
    "\n",
    "# Model interpretability\n",
    "import shap\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon, friedmanchisquare\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Model evaluation environment setup completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192bb4c7",
   "metadata": {},
   "source": [
    "## üîÑ Load Trained Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8014f5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MLflow experiment results\n",
    "mlflow.set_tracking_uri(\"file:../mlruns\")\n",
    "\n",
    "try:\n",
    "    # Get experiment\n",
    "    experiment = mlflow.get_experiment_by_name(\"hotel_cancellation_prediction\")\n",
    "    \n",
    "    if experiment:\n",
    "        experiment_id = experiment.experiment_id\n",
    "        runs = mlflow.search_runs(experiment_ids=[experiment_id])\n",
    "        \n",
    "        print(f\"‚úÖ Found {len(runs)} experiment runs\")\n",
    "        print(f\"üìä EXPERIMENT OVERVIEW:\")\n",
    "        \n",
    "        # Display key metrics\n",
    "        metrics_cols = [col for col in runs.columns if col.startswith('metrics.')]\n",
    "        if metrics_cols:\n",
    "            display_runs = runs[['tags.mlflow.runName'] + metrics_cols].head()\n",
    "            print(display_runs.to_string(index=False))\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No MLflow experiments found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load MLflow results: {e}\")\n",
    "    print(\"Creating sample evaluation data for demonstration...\")\n",
    "    \n",
    "    # Create sample results\n",
    "    runs = pd.DataFrame({\n",
    "        'tags.mlflow.runName': ['LogisticRegression', 'RandomForest', 'XGBoost', 'PyTorch_MLP'],\n",
    "        'metrics.test_accuracy': [0.825, 0.867, 0.889, 0.871],\n",
    "        'metrics.test_f1': [0.756, 0.823, 0.850, 0.831],\n",
    "        'metrics.test_roc_auc': [0.891, 0.932, 0.958, 0.941]\n",
    "    })\n",
    "    print(\"Sample evaluation results created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaa7dca",
   "metadata": {},
   "source": [
    "## üìà Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535506cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance comparison\n",
    "print(\"üìà MODEL PERFORMANCE COMPARISON:\")\n",
    "\n",
    "if 'runs' in locals() and not runs.empty:\n",
    "    # Extract performance metrics\n",
    "    performance_data = []\n",
    "    \n",
    "    for idx, run in runs.iterrows():\n",
    "        model_name = run.get('tags.mlflow.runName', f'Model_{idx}')\n",
    "        \n",
    "        performance_data.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': run.get('metrics.test_accuracy', 0),\n",
    "            'F1-Score': run.get('metrics.test_f1', 0),\n",
    "            'ROC-AUC': run.get('metrics.test_roc_auc', 0),\n",
    "            'Precision': run.get('metrics.test_precision', 0),\n",
    "            'Recall': run.get('metrics.test_recall', 0)\n",
    "        })\n",
    "    \n",
    "    performance_df = pd.DataFrame(performance_data)\n",
    "    performance_df = performance_df.sort_values('F1-Score', ascending=False)\n",
    "    \n",
    "    print(\"\\nüèÜ PERFORMANCE RANKING (by F1-Score):\")\n",
    "    print(performance_df.round(4))\n",
    "    \n",
    "    # Visualize performance comparison\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # F1-Score comparison\n",
    "    axes[0].bar(performance_df['Model'], performance_df['F1-Score'])\n",
    "    axes[0].set_title('F1-Score Comparison')\n",
    "    axes[0].set_ylabel('F1-Score')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # ROC-AUC comparison\n",
    "    axes[1].bar(performance_df['Model'], performance_df['ROC-AUC'])\n",
    "    axes[1].set_title('ROC-AUC Comparison')\n",
    "    axes[1].set_ylabel('ROC-AUC')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    axes[2].bar(performance_df['Model'], performance_df['Accuracy'])\n",
    "    axes[2].set_title('Accuracy Comparison')\n",
    "    axes[2].set_ylabel('Accuracy')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Champion model identification\n",
    "    champion_model = performance_df.iloc[0]['Model']\n",
    "    champion_f1 = performance_df.iloc[0]['F1-Score']\n",
    "    print(f\"\\nü•á CHAMPION MODEL: {champion_model} (F1-Score: {champion_f1:.4f})\")\n",
    "\n",
    "else:\n",
    "    print(\"No performance data available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc2f65",
   "metadata": {},
   "source": [
    "## üîç Model Interpretability with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515ad725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP-based model interpretability\n",
    "print(\"üîç MODEL INTERPRETABILITY ANALYSIS:\")\n",
    "\n",
    "# Load champion model for SHAP analysis\n",
    "try:\n",
    "    # Attempt to load XGBoost model (typical champion)\n",
    "    model_path = \"../models/xgboost_model.pkl\"\n",
    "    champion_model_obj = joblib.load(model_path)\n",
    "    print(f\"‚úÖ Champion model loaded from {model_path}\")\n",
    "    \n",
    "    # Load test data for SHAP analysis\n",
    "    # This would typically come from the feature engineering stage\n",
    "    print(\"SHAP analysis ready - load test data to continue...\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Champion model not found\")\n",
    "    print(\"SHAP analysis will be implemented once models are trained\")\n",
    "\n",
    "# SHAP implementation will include:\n",
    "# - Feature importance ranking\n",
    "# - SHAP value distributions\n",
    "# - Individual prediction explanations\n",
    "# - Business impact interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db92cd0",
   "metadata": {},
   "source": [
    "## üíº Business Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8949ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business impact analysis\n",
    "print(\"üíº BUSINESS IMPACT ANALYSIS:\")\n",
    "\n",
    "# Revenue impact simulation based on model predictions\n",
    "# This will include:\n",
    "# - Cancellation cost reduction estimates\n",
    "# - Overbooking optimization potential\n",
    "# - Market segment insights for Sri Lankan tourism\n",
    "# - Seasonal booking strategies\n",
    "\n",
    "print(\"Business impact analysis to be implemented...\")\n",
    "print(\"Focus areas:\")\n",
    "print(\"- Revenue loss prevention through early cancellation detection\")\n",
    "print(\"- Overbooking optimization strategies\")\n",
    "print(\"- Customer segmentation for targeted marketing\")\n",
    "print(\"- Seasonal demand forecasting for Sri Lankan market\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a15dfe8",
   "metadata": {},
   "source": [
    "## üìä Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae2e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical significance testing for model comparison\n",
    "print(\"üìä STATISTICAL SIGNIFICANCE TESTING:\")\n",
    "\n",
    "# Implement statistical tests for academic rigor:\n",
    "# - Wilcoxon signed-rank test for paired model comparison\n",
    "# - Friedman test for multiple model comparison\n",
    "# - Effect size calculations\n",
    "\n",
    "print(\"Statistical testing framework to be implemented...\")\n",
    "print(\"Tests to include:\")\n",
    "print(\"- Wilcoxon signed-rank test for pairwise model comparison\")\n",
    "print(\"- Friedman test for multiple model ranking\")\n",
    "print(\"- Effect size calculations for practical significance\")\n",
    "print(\"- Confidence intervals for performance metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e25e949",
   "metadata": {},
   "source": [
    "## üìù Academic Documentation Summary\n",
    "\n",
    "**Model Evaluation Completion Checklist:**\n",
    "\n",
    "- ‚úÖ Environment setup and model loading\n",
    "- ‚úÖ Performance metrics comparison framework\n",
    "- ‚úÖ Champion model identification process\n",
    "- ‚ñ° SHAP-based feature importance analysis\n",
    "- ‚ñ° Model interpretability and business insights\n",
    "- ‚ñ° Statistical significance testing\n",
    "- ‚ñ° Business impact quantification\n",
    "- ‚ñ° Sri Lankan tourism market recommendations\n",
    "- ‚ñ° Academic methodology validation\n",
    "- ‚ñ° Final research conclusions and future work\n",
    "\n",
    "**Expected Academic Outcomes:**\n",
    "- Rigorous model comparison with statistical validation\n",
    "- Comprehensive interpretability analysis using SHAP\n",
    "- Business-relevant insights for hospitality industry\n",
    "- Academic-standard documentation for NIB 7072 coursework\n",
    "- Actionable recommendations for Sri Lankan tourism market"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
