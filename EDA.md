# Comprehensive Exploratory Data Analysis (EDA) Instructions
## Hotel Booking Cancellation Prediction - Academic Research Framework

This document provides detailed instructions for conducting a thorough EDA for the hotel booking cancellation prediction project, aligned with NIB 7072 academic standards and Sri Lankan tourism market applications.

## üéØ Research Objectives & Academic Context

### Primary Goals
- **Academic Rigor**: Follow university-level analytical standards with proper statistical methodology
- **Business Intelligence**: Generate actionable insights for Sri Lankan hospitality sector
- **Feature Engineering**: Identify predictive variables for cancellation risk assessment
- **Market Analysis**: Understand booking patterns relevant to South Asian tourism dynamics

### Key Research Questions
1. What are the primary factors driving booking cancellations?
2. How do seasonal patterns affect cancellation rates in hospitality?
3. Which customer segments pose highest cancellation risk?
4. What pricing strategies minimize cancellation probability?
5. How do lead times correlate with cancellation behavior?

---

## üìä Phase 1: Dataset Overview & Initial Exploration

### 1.1 Data Import and Initial Assessment

```python
# Essential libraries for comprehensive EDA
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

# Academic visualization settings
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
```

### 1.2 Dataset Structure Analysis

**Execute these steps systematically:**

1. **Load and examine dataset dimensions**
   ```python
   df = pd.read_csv('data/raw/hotel_bookings.csv')
   print(f"Dataset Shape: {df.shape}")
   print(f"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
   ```

2. **Comprehensive data overview**
   ```python
   # Display first 10 rows with all columns visible
   df.head(10)
   
   # Data types and memory optimization opportunities
   df.info(memory_usage='deep')
   
   # Statistical summary for numerical variables
   df.describe(include='all').round(2)
   ```

3. **Target variable distribution analysis**
   ```python
   # Cancellation rate - critical business metric
   cancellation_rate = df['is_canceled'].mean()
   print(f"Overall Cancellation Rate: {cancellation_rate:.3f} ({cancellation_rate*100:.1f}%)")
   
   # Visual representation
   fig, ax = plt.subplots(1, 2, figsize=(12, 5))
   
   # Count plot
   sns.countplot(data=df, x='is_canceled', ax=ax[0])
   ax[0].set_title('Booking Status Distribution')
   ax[0].set_xlabel('Is Canceled (0=No, 1=Yes)')
   
   # Pie chart
   df['is_canceled'].value_counts().plot(kind='pie', ax=ax[1], autopct='%1.1f%%')
   ax[1].set_title('Cancellation Rate Distribution')
   ```

---

## üìà Phase 2: Data Quality Assessment

### 2.1 Missing Data Analysis

**Comprehensive missing value investigation:**

```python
# Missing value summary with business impact assessment
missing_data = pd.DataFrame({
    'Column': df.columns,
    'Missing_Count': df.isnull().sum(),
    'Missing_Percentage': (df.isnull().sum() / len(df)) * 100,
    'Data_Type': df.dtypes
})
missing_data = missing_data[missing_data['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)

# Visualize missing patterns
import missingno as msno
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

msno.bar(df, ax=axes[0,0])
axes[0,0].set_title('Missing Value Counts')

msno.matrix(df, ax=axes[0,1])
axes[0,1].set_title('Missing Value Patterns')

msno.heatmap(df, ax=axes[1,0])
axes[1,0].set_title('Missing Value Correlations')

msno.dendrogram(df, ax=axes[1,1])
axes[1,1].set_title('Missing Value Hierarchical Clustering')

plt.tight_layout()
plt.show()
```

### 2.2 Reservation Status and Date Consistency Validation

**Critical business logic validation for reservation data:**

```python
def validate_reservation_consistency(df):
    """
    Validate consistency between reservation_status, reservation_status_date, and is_canceled
    """
    print("üîç RESERVATION STATUS CONSISTENCY ANALYSIS:")
    
    # Check reservation status vs cancellation flag
    if 'reservation_status' in df.columns:
        status_vs_cancel = pd.crosstab(df['reservation_status'], df['is_canceled'], margins=True)
        print("\nüìä Reservation Status vs Cancellation Cross-tabulation:")
        print(status_vs_cancel)
        
        # Calculate consistency percentage
        canceled_records = df[df['is_canceled'] == 1]
        if len(canceled_records) > 0:
            canceled_status_match = (canceled_records['reservation_status'] == 'Canceled').sum()
            consistency_rate = canceled_status_match / len(canceled_records) * 100
            print(f"\n‚úÖ Consistency Rate: {consistency_rate:.1f}% of canceled bookings have 'Canceled' status")
        
        # Visualize status distribution
        plt.figure(figsize=(12, 5))
        
        plt.subplot(1, 2, 1)
        df['reservation_status'].value_counts().plot(kind='bar')
        plt.title('Reservation Status Distribution')
        plt.xticks(rotation=45)
        
        plt.subplot(1, 2, 2)
        status_cancel_rate = df.groupby('reservation_status')['is_canceled'].mean()
        status_cancel_rate.plot(kind='bar')
        plt.title('Cancellation Rate by Reservation Status')
        plt.ylabel('Cancellation Rate')
        plt.xticks(rotation=45)
        
        plt.tight_layout()
        plt.show()
    
    # Analyze reservation status date patterns
    if 'reservation_status_date' in df.columns:
        print(f"\nüìÖ Reservation Status Date Analysis:")
        
        # Convert to datetime if not already
        df_temp = df.copy()
        try:
            df_temp['reservation_status_date'] = pd.to_datetime(df_temp['reservation_status_date'])
            
            # Check if canceled bookings have status dates
            canceled_with_date = df_temp[
                (df_temp['is_canceled'] == 1) & 
                (df_temp['reservation_status_date'].notna())
            ]
            
            print(f"Canceled bookings with status dates: {len(canceled_with_date)} / {df_temp['is_canceled'].sum()}")
            
            # Analyze date patterns for canceled bookings
            if len(canceled_with_date) > 0:
                print(f"Date range for canceled bookings: {canceled_with_date['reservation_status_date'].min()} to {canceled_with_date['reservation_status_date'].max()}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Could not analyze reservation dates: {e}")
    
    return status_vs_cancel if 'reservation_status' in df.columns else None

# Perform reservation consistency validation
reservation_analysis = validate_reservation_consistency(df)
```

### 2.3 High Missing Value Column Assessment

**Systematic analysis of agent and company columns for potential removal:**

```python
def assess_high_missing_columns(df):
    """
    Detailed analysis of columns with high missing percentages
    """
    print("üéØ HIGH MISSING VALUE COLUMN ASSESSMENT:")
    
    # Calculate missing percentages for all columns
    missing_percentages = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False)
    high_missing = missing_percentages[missing_percentages > 50]  # More than 50% missing
    
    print(f"\nüìä Columns with >50% missing values:")
    for col, pct in high_missing.items():
        print(f"  ‚Ä¢ {col}: {pct:.1f}% missing")
    
    # Specific analysis for agent and company columns
    problem_columns = ['agent', 'company']
    
    for col in problem_columns:
        if col in df.columns:
            missing_pct = df[col].isnull().sum() / len(df) * 100
            print(f"\nüîç {col.upper()} COLUMN ANALYSIS:")
            print(f"Missing percentage: {missing_pct:.1f}%")
            print(f"Unique values: {df[col].nunique()}")
            print(f"Non-null unique values: {df[col].dropna().nunique()}")
            
            # Check if missing values correlate with cancellations
            missing_mask = df[col].isnull()
            cancel_rate_missing = df[missing_mask]['is_canceled'].mean()
            cancel_rate_present = df[~missing_mask]['is_canceled'].mean()
            
            print(f"Cancellation rate when {col} is missing: {cancel_rate_missing:.3f}")
            print(f"Cancellation rate when {col} is present: {cancel_rate_present:.3f}")
            
            # Statistical test for difference
            from scipy.stats import chi2_contingency
            contingency = pd.crosstab(missing_mask, df['is_canceled'])
            chi2, p_value, _, _ = chi2_contingency(contingency)
            print(f"Chi-square test p-value: {p_value:.6f}")
            
            if p_value < 0.05:
                print(f"‚úÖ Significant relationship between {col} missingness and cancellation")
            else:
                print(f"‚ùå No significant relationship - safe to drop {col}")
    
    # Recommendation for column removal
    print(f"\nüìã COLUMN REMOVAL RECOMMENDATIONS:")
    
    # Analyze business impact of dropping columns
    columns_to_drop = ['agent', 'company', 'country', 'reservation_status', 
                      'reservation_status_date', 'booking_changes']
    
    for col in columns_to_drop:
        if col in df.columns:
            missing_pct = df[col].isnull().sum() / len(df) * 100
            unique_vals = df[col].nunique()
            
            if col == 'country':
                reason = f"High cardinality ({unique_vals} values) - difficult to encode effectively"
            elif col in ['reservation_status', 'reservation_status_date']:
                reason = "Data leakage risk - contains information about cancellation outcome"
            elif col == 'booking_changes':
                reason = "Low predictive value and potential data leakage"
            elif missing_pct > 70:
                reason = f"Excessive missing values ({missing_pct:.1f}%)"
            else:
                reason = "Business logic suggests limited predictive value"
            
            print(f"  ‚Ä¢ {col}: {reason}")
    
    return high_missing, problem_columns

# Perform high missing value assessment
high_missing_analysis, problem_cols = assess_high_missing_columns(df)
```

### 2.4 Data Quality Issues Identification

**Critical data quality checks:**

1. **Logical inconsistencies**
   ```python
   # Check for impossible combinations
   impossible_bookings = df[(df['adults'] == 0) & (df['children'] == 0) & (df['babies'] == 0)]
   print(f"Bookings with no guests: {len(impossible_bookings)}")
   
   # Negative values where they shouldn't exist
   for col in ['adults', 'children', 'babies', 'stays_in_weekend_nights', 'stays_in_week_nights']:
       negative_values = (df[col] < 0).sum()
       if negative_values > 0:
           print(f"Negative values in {col}: {negative_values}")
   ```

2. **Outlier detection using IQR method**
   ```python
   numerical_cols = df.select_dtypes(include=[np.number]).columns
   outlier_summary = {}
   
   for col in numerical_cols:
       Q1 = df[col].quantile(0.25)
       Q3 = df[col].quantile(0.75)
       IQR = Q3 - Q1
       lower_bound = Q1 - 1.5 * IQR
       upper_bound = Q3 + 1.5 * IQR
       
       outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
       outlier_summary[col] = {
           'count': len(outliers),
           'percentage': len(outliers) / len(df) * 100,
           'lower_bound': lower_bound,
           'upper_bound': upper_bound
       }
   
   outlier_df = pd.DataFrame(outlier_summary).T
   print(outlier_df.round(2))
   ```

### 2.5 Lead Time Impact Analysis

**Preliminary investigation of lead time as cancellation predictor:**

```python
def analyze_lead_time_impact(df):
    """
    Detailed analysis of lead time patterns and cancellation relationship
    """
    print("‚è∞ LEAD TIME IMPACT ANALYSIS:")
    
    # Basic lead time statistics
    print(f"\nüìä Lead Time Statistics:")
    print(df['lead_time'].describe())
    
    # Lead time vs cancellation relationship
    canceled_lead_time = df[df['is_canceled'] == 1]['lead_time']
    not_canceled_lead_time = df[df['is_canceled'] == 0]['lead_time']
    
    print(f"\nüìà Lead Time by Cancellation Status:")
    print(f"Average lead time (canceled): {canceled_lead_time.mean():.1f} days")
    print(f"Average lead time (not canceled): {not_canceled_lead_time.mean():.1f} days")
    print(f"Median lead time (canceled): {canceled_lead_time.median():.1f} days")
    print(f"Median lead time (not canceled): {not_canceled_lead_time.median():.1f} days")
    
    # Statistical significance test
    from scipy.stats import mannwhitneyu
    statistic, p_value = mannwhitneyu(canceled_lead_time, not_canceled_lead_time, alternative='two-sided')
    print(f"\nMann-Whitney U test p-value: {p_value:.6f}")
    
    # Visualization
    plt.figure(figsize=(15, 10))
    
    # Lead time distribution by cancellation status
    plt.subplot(2, 3, 1)
    plt.hist([not_canceled_lead_time, canceled_lead_time], 
             bins=50, alpha=0.7, label=['Not Canceled', 'Canceled'])
    plt.xlabel('Lead Time (days)')
    plt.ylabel('Frequency')
    plt.title('Lead Time Distribution by Cancellation Status')
    plt.legend()
    
    # Box plot comparison
    plt.subplot(2, 3, 2)
    df.boxplot(column='lead_time', by='is_canceled', ax=plt.gca())
    plt.title('Lead Time by Cancellation Status')
    plt.suptitle('')  # Remove automatic title
    
    # Lead time bins analysis
    plt.subplot(2, 3, 3)
    lead_time_bins = pd.cut(df['lead_time'], 
                           bins=[0, 7, 30, 90, 180, 365, df['lead_time'].max()],
                           labels=['0-7 days', '8-30 days', '31-90 days', 
                                  '91-180 days', '181-365 days', '365+ days'])
    
    cancellation_by_bins = df.groupby(lead_time_bins)['is_canceled'].agg(['mean', 'count'])
    cancellation_by_bins['mean'].plot(kind='bar')
    plt.title('Cancellation Rate by Lead Time Bins')
    plt.ylabel('Cancellation Rate')
    plt.xticks(rotation=45)
    
    # Correlation analysis
    plt.subplot(2, 3, 4)
    correlation = df['lead_time'].corr(df['is_canceled'])
    plt.scatter(df['lead_time'], df['is_canceled'], alpha=0.1)
    plt.xlabel('Lead Time (days)')
    plt.ylabel('Is Canceled')
    plt.title(f'Lead Time vs Cancellation\n(Correlation: {correlation:.3f})')
    
    # Lead time percentiles for each group
    plt.subplot(2, 3, 5)
    percentiles = [10, 25, 50, 75, 90]
    canceled_percentiles = [canceled_lead_time.quantile(p/100) for p in percentiles]
    not_canceled_percentiles = [not_canceled_lead_time.quantile(p/100) for p in percentiles]
    
    x = range(len(percentiles))
    plt.plot(x, canceled_percentiles, 'ro-', label='Canceled', linewidth=2)
    plt.plot(x, not_canceled_percentiles, 'bo-', label='Not Canceled', linewidth=2)
    plt.xticks(x, [f'P{p}' for p in percentiles])
    plt.ylabel('Lead Time (days)')
    plt.title('Lead Time Percentiles by Cancellation Status')
    plt.legend()
    
    # Monthly lead time patterns
    plt.subplot(2, 3, 6)
    monthly_lead_time = df.groupby('arrival_month')['lead_time'].mean()
    monthly_cancel_rate = df.groupby('arrival_month')['is_canceled'].mean()
    
    ax1 = plt.gca()
    color = 'tab:blue'
    ax1.set_xlabel('Arrival Month')
    ax1.set_ylabel('Average Lead Time (days)', color=color)
    ax1.plot(monthly_lead_time.index, monthly_lead_time.values, 'o-', color=color)
    ax1.tick_params(axis='y', labelcolor=color)
    
    ax2 = ax1.twinx()
    color = 'tab:red'
    ax2.set_ylabel('Cancellation Rate', color=color)
    ax2.plot(monthly_cancel_rate.index, monthly_cancel_rate.values, 's-', color=color)
    ax2.tick_params(axis='y', labelcolor=color)
    
    plt.title('Lead Time vs Cancellation Rate by Month')
    
    plt.tight_layout()
    plt.show()
    
    return cancellation_by_bins, correlation

# Perform lead time impact analysis
lead_time_analysis, lead_correlation = analyze_lead_time_impact(df)
```

---

## üîç Phase 3: Univariate Analysis

### 3.1 Numerical Variables Deep Dive

**For each numerical variable, create comprehensive analysis:**

```python
numerical_features = ['lead_time', 'stays_in_weekend_nights', 'stays_in_week_nights', 
                     'adults', 'children', 'babies', 'previous_cancellations',
                     'previous_bookings_not_canceled', 'booking_changes', 'adr',
                     'days_in_waiting_list', 'required_car_parking_spaces',
                     'total_of_special_requests']

# Create comprehensive numerical analysis
fig, axes = plt.subplots(len(numerical_features), 4, figsize=(20, 5*len(numerical_features)))

for i, feature in enumerate(numerical_features):
    # Distribution plot
    sns.histplot(data=df, x=feature, kde=True, ax=axes[i,0])
    axes[i,0].set_title(f'{feature} - Distribution')
    
    # Box plot
    sns.boxplot(data=df, y=feature, ax=axes[i,1])
    axes[i,1].set_title(f'{feature} - Box Plot')
    
    # Q-Q plot for normality assessment
    from scipy import stats
    stats.probplot(df[feature].dropna(), dist="norm", plot=axes[i,2])
    axes[i,2].set_title(f'{feature} - Q-Q Plot')
    
    # Violin plot by cancellation status
    sns.violinplot(data=df, x='is_canceled', y=feature, ax=axes[i,3])
    axes[i,3].set_title(f'{feature} vs Cancellation')

plt.tight_layout()
plt.show()
```

**Statistical Summary for Each Feature:**
```python
for feature in numerical_features:
    print(f"\n=== {feature.upper()} ANALYSIS ===")
    print(f"Mean: {df[feature].mean():.2f}")
    print(f"Median: {df[feature].median():.2f}")
    print(f"Mode: {df[feature].mode().iloc[0] if not df[feature].mode().empty else 'No mode'}")
    print(f"Std Dev: {df[feature].std():.2f}")
    print(f"Skewness: {df[feature].skew():.2f}")
    print(f"Kurtosis: {df[feature].kurtosis():.2f}")
    print(f"Range: {df[feature].min():.2f} to {df[feature].max():.2f}")
    
    # Business interpretation
    if feature == 'lead_time':
        print("üìä Business Insight: Lead time distribution indicates booking behavior patterns")
    elif feature == 'adr':
        print("üí∞ Business Insight: Average daily rate shows pricing strategy effectiveness")
```

### 3.2 Categorical Variables Analysis

**Systematic categorical analysis:**

```python
categorical_features = ['hotel', 'arrival_date_month', 'meal', 'country', 
                       'market_segment', 'distribution_channel', 'is_repeated_guest',
                       'reserved_room_type', 'assigned_room_type', 'deposit_type',
                       'customer_type', 'reservation_status']

# Comprehensive categorical analysis
for feature in categorical_features:
    print(f"\n=== {feature.upper()} ANALYSIS ===")
    
    # Frequency analysis
    value_counts = df[feature].value_counts()
    print(f"Unique values: {df[feature].nunique()}")
    print(f"Top 5 categories:")
    print(value_counts.head())
    
    # Create visualization
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    # Count plot
    if df[feature].nunique() <= 10:
        sns.countplot(data=df, x=feature, ax=axes[0])
        axes[0].tick_params(axis='x', rotation=45)
    else:
        # For high cardinality, show top 10
        top_categories = value_counts.head(10)
        top_categories.plot(kind='bar', ax=axes[0])
        axes[0].set_title(f'Top 10 {feature} Categories')
    
    # Cancellation rate by category
    cancel_rate = df.groupby(feature)['is_canceled'].mean()
    if len(cancel_rate) <= 15:
        cancel_rate.plot(kind='bar', ax=axes[1])
        axes[1].set_title(f'Cancellation Rate by {feature}')
        axes[1].set_ylabel('Cancellation Rate')
        axes[1].tick_params(axis='x', rotation=45)
    
    # Pie chart for proportions
    if df[feature].nunique() <= 8:
        value_counts.plot(kind='pie', ax=axes[2], autopct='%1.1f%%')
        axes[2].set_title(f'{feature} Distribution')
    
    plt.tight_layout()
    plt.show()
```

---

## üìä Phase 4: Bivariate Analysis

### 4.1 Feature Relationships with Target Variable

**Correlation analysis with business interpretation:**

```python
# Correlation matrix focusing on cancellation prediction
correlation_matrix = df.corr()
cancellation_correlations = correlation_matrix['is_canceled'].abs().sort_values(ascending=False)

print("üéØ TOP FEATURES CORRELATED WITH CANCELLATIONS:")
print(cancellation_correlations.head(10))

# Heatmap visualization
plt.figure(figsize=(12, 10))
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdYlBu_r', center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})
plt.title('Feature Correlation Heatmap')
plt.show()
```

### 4.2 Critical Business Relationships

**Revenue impact analysis:**

```python
# 1. Lead time vs Cancellation analysis
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
sns.boxplot(data=df, x='is_canceled', y='lead_time')
plt.title('Lead Time Distribution by Cancellation Status')

plt.subplot(1, 3, 2)
# Create lead time bins for analysis
df['lead_time_category'] = pd.cut(df['lead_time'], 
                                 bins=[0, 30, 60, 120, 365, 1000], 
                                 labels=['0-30 days', '31-60 days', '61-120 days', '121-365 days', '365+ days'])
lead_time_cancel = df.groupby('lead_time_category')['is_canceled'].mean()
lead_time_cancel.plot(kind='bar')
plt.title('Cancellation Rate by Lead Time Category')
plt.xticks(rotation=45)

plt.subplot(1, 3, 3)
# Revenue impact
revenue_impact = df.groupby(['lead_time_category', 'is_canceled']).agg({
    'adr': 'mean'
}).reset_index()
sns.barplot(data=revenue_impact, x='lead_time_category', y='adr', hue='is_canceled')
plt.title('Average Daily Rate by Lead Time and Cancellation')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()
```

**Seasonal patterns analysis:**

```python
# Monthly booking and cancellation patterns
monthly_analysis = df.groupby('arrival_date_month').agg({
    'is_canceled': ['count', 'sum', 'mean'],
    'adr': 'mean'
}).round(2)

monthly_analysis.columns = ['Total_Bookings', 'Total_Cancellations', 'Cancellation_Rate', 'Avg_Rate']

# Sort months chronologically
month_order = ['January', 'February', 'March', 'April', 'May', 'June',
               'July', 'August', 'September', 'October', 'November', 'December']
monthly_analysis = monthly_analysis.reindex(month_order)

# Visualization
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

monthly_analysis['Total_Bookings'].plot(kind='line', marker='o', ax=axes[0,0])
axes[0,0].set_title('Monthly Booking Volume')
axes[0,0].tick_params(axis='x', rotation=45)

monthly_analysis['Cancellation_Rate'].plot(kind='line', marker='o', color='red', ax=axes[0,1])
axes[0,1].set_title('Monthly Cancellation Rate')
axes[0,1].tick_params(axis='x', rotation=45)

monthly_analysis['Avg_Rate'].plot(kind='line', marker='o', color='green', ax=axes[1,0])
axes[1,0].set_title('Monthly Average Daily Rate')
axes[1,0].tick_params(axis='x', rotation=45)

# Combined view
ax2 = axes[1,1]
ax3 = ax2.twinx()
ax2.plot(monthly_analysis.index, monthly_analysis['Cancellation_Rate'], 'r-', marker='o', label='Cancellation Rate')
ax3.plot(monthly_analysis.index, monthly_analysis['Avg_Rate'], 'g-', marker='s', label='Avg Daily Rate')
ax2.set_ylabel('Cancellation Rate', color='r')
ax3.set_ylabel('Average Daily Rate', color='g')
ax2.tick_params(axis='x', rotation=45)
plt.title('Cancellation Rate vs Pricing Strategy')

plt.tight_layout()
plt.show()
```

### 4.3 Mean Encoding Analysis for Categorical Variables

**Analyze categorical variable encoding strategies with target-based statistics:**

```python
def analyze_mean_encoding_potential(df):
    """
    Analyze categorical variables for mean encoding potential and effectiveness
    """
    print("üéØ MEAN ENCODING ANALYSIS FOR CATEGORICAL VARIABLES:")
    
    categorical_columns = ['hotel', 'meal', 'market_segment', 'distribution_channel', 
                          'reserved_room_type', 'assigned_room_type', 'deposit_type', 
                          'customer_type', 'arrival_date_month']
    
    mean_encoding_results = {}
    
    plt.figure(figsize=(20, 15))
    plot_idx = 1
    
    for col in categorical_columns:
        if col in df.columns:
            # Calculate mean encoding values
            mean_encoded = df.groupby(col)['is_canceled'].agg(['mean', 'count', 'std']).reset_index()
            mean_encoded.columns = [col, 'cancellation_rate', 'count', 'std']
            mean_encoded = mean_encoded.sort_values('cancellation_rate', ascending=False)
            
            # Calculate encoding effectiveness metrics
            overall_mean = df['is_canceled'].mean()
            variance_explained = np.var(mean_encoded['cancellation_rate'])
            total_variance = overall_mean * (1 - overall_mean)  # For binary target
            effectiveness_ratio = variance_explained / total_variance if total_variance > 0 else 0
            
            # Calculate statistical significance
            from scipy.stats import f_oneway
            groups = [df[df[col] == cat]['is_canceled'].values for cat in df[col].unique()]
            f_stat, p_value = f_oneway(*groups)
            
            mean_encoding_results[col] = {
                'encoding_table': mean_encoded,
                'effectiveness_ratio': effectiveness_ratio,
                'f_statistic': f_stat,
                'p_value': p_value,
                'unique_values': len(mean_encoded),
                'min_cancellation_rate': mean_encoded['cancellation_rate'].min(),
                'max_cancellation_rate': mean_encoded['cancellation_rate'].max(),
                'rate_range': mean_encoded['cancellation_rate'].max() - mean_encoded['cancellation_rate'].min()
            }
            
            print(f"\nüìä {col.upper()} MEAN ENCODING ANALYSIS:")
            print(f"Unique categories: {len(mean_encoded)}")
            print(f"Cancellation rate range: {mean_encoded['cancellation_rate'].min():.3f} - {mean_encoded['cancellation_rate'].max():.3f}")
            print(f"Rate spread: {mean_encoded['cancellation_rate'].max() - mean_encoded['cancellation_rate'].min():.3f}")
            print(f"Effectiveness ratio: {effectiveness_ratio:.3f}")
            print(f"F-statistic p-value: {p_value:.6f}")
            
            # Display top categories
            print(f"Top 5 categories by cancellation rate:")
            print(mean_encoded.head().to_string(index=False))
            
            # Visualization
            if plot_idx <= 9:  # Only plot first 9 for space
                plt.subplot(3, 3, plot_idx)
                
                # Bar plot with error bars
                x_pos = range(len(mean_encoded))
                colors = ['red' if rate > overall_mean else 'blue' for rate in mean_encoded['cancellation_rate']]
                
                plt.bar(x_pos, mean_encoded['cancellation_rate'], 
                       color=colors, alpha=0.7, 
                       yerr=mean_encoded['std']/np.sqrt(mean_encoded['count']), 
                       capsize=3)
                
                plt.axhline(y=overall_mean, color='black', linestyle='--', alpha=0.7, 
                           label=f'Overall Mean ({overall_mean:.3f})')
                
                plt.xlabel(f'{col} (sorted by cancellation rate)')
                plt.ylabel('Cancellation Rate')
                plt.title(f'{col} - Mean Encoding\n(Range: {mean_encoded["cancellation_rate"].max() - mean_encoded["cancellation_rate"].min():.3f})')
                plt.xticks(x_pos, mean_encoded[col], rotation=45, ha='right')
                plt.legend()
                
                plot_idx += 1
    
    plt.tight_layout()
    plt.show()
    
    # Summary of encoding effectiveness
    print(f"\nüèÜ MEAN ENCODING EFFECTIVENESS RANKING:")
    effectiveness_ranking = sorted(mean_encoding_results.items(), 
                                 key=lambda x: x[1]['effectiveness_ratio'], reverse=True)
    
    for i, (col, results) in enumerate(effectiveness_ranking, 1):
        significance = "***" if results['p_value'] < 0.001 else "**" if results['p_value'] < 0.01 else "*" if results['p_value'] < 0.05 else ""
        print(f"{i:2d}. {col:20s} | Effectiveness: {results['effectiveness_ratio']:.4f} | "
              f"Range: {results['rate_range']:.3f} | p-value: {results['p_value']:.6f} {significance}")
    
    # Recommendations
    print(f"\nüí° MEAN ENCODING RECOMMENDATIONS:")
    
    high_effectiveness = [col for col, results in mean_encoding_results.items() 
                         if results['effectiveness_ratio'] > 0.1 and results['p_value'] < 0.05]
    
    medium_effectiveness = [col for col, results in mean_encoding_results.items() 
                           if 0.05 <= results['effectiveness_ratio'] <= 0.1 and results['p_value'] < 0.05]
    
    low_effectiveness = [col for col, results in mean_encoding_results.items() 
                        if results['effectiveness_ratio'] < 0.05 or results['p_value'] >= 0.05]
    
    print(f"‚úÖ HIGH PRIORITY for mean encoding: {high_effectiveness}")
    print(f"üî∂ MEDIUM PRIORITY for mean encoding: {medium_effectiveness}")
    print(f"‚ùå LOW PRIORITY for mean encoding: {low_effectiveness}")
    
    return mean_encoding_results

# Perform mean encoding analysis
mean_encoding_analysis = analyze_mean_encoding_potential(df)
```

---

## üè® Phase 5: Hotel-Specific Analysis

### 5.1 Comparative Hotel Performance

**Resort vs City Hotel analysis:**

```python
# Hotel type comparison
hotel_comparison = df.groupby(['hotel', 'is_canceled']).size().unstack()
hotel_comparison['total'] = hotel_comparison.sum(axis=1)
hotel_comparison['cancellation_rate'] = hotel_comparison[1] / hotel_comparison['total']

print("üè® HOTEL PERFORMANCE COMPARISON:")
print(hotel_comparison)

# Revenue analysis by hotel type
hotel_revenue = df.groupby(['hotel', 'is_canceled']).agg({
    'adr': ['mean', 'sum', 'count']
}).round(2)

# Visualization
fig, axes = plt.subplots(2, 3, figsize=(18, 10))

# Cancellation rates
hotel_comparison['cancellation_rate'].plot(kind='bar', ax=axes[0,0])
axes[0,0].set_title('Cancellation Rate by Hotel Type')
axes[0,0].set_ylabel('Cancellation Rate')

# Average daily rates
sns.boxplot(data=df, x='hotel', y='adr', hue='is_canceled', ax=axes[0,1])
axes[0,1].set_title('ADR Distribution by Hotel and Cancellation Status')

# Lead time patterns
sns.violinplot(data=df, x='hotel', y='lead_time', hue='is_canceled', ax=axes[0,2])
axes[0,2].set_title('Lead Time Patterns by Hotel Type')

# Market segment preferences
hotel_market = pd.crosstab(df['hotel'], df['market_segment'])
hotel_market_pct = hotel_market.div(hotel_market.sum(axis=1), axis=0)
sns.heatmap(hotel_market_pct, annot=True, fmt='.2f', ax=axes[1,0])
axes[1,0].set_title('Market Segment Distribution by Hotel')

# Room type preferences
hotel_room = pd.crosstab(df['hotel'], df['reserved_room_type'])
hotel_room_pct = hotel_room.div(hotel_room.sum(axis=1), axis=0)
sns.heatmap(hotel_room_pct, annot=True, fmt='.2f', ax=axes[1,1])
axes[1,1].set_title('Room Type Preferences by Hotel')

# Seasonal patterns
hotel_seasonal = df.groupby(['hotel', 'arrival_date_month'])['is_canceled'].mean().reset_index()
hotel_seasonal_pivot = hotel_seasonal.pivot(index='arrival_date_month', columns='hotel', values='is_canceled')
hotel_seasonal_pivot.plot(kind='line', marker='o', ax=axes[1,2])
axes[1,2].set_title('Seasonal Cancellation Patterns')
axes[1,2].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()
```

---

## üåç Phase 6: Geographic and Market Analysis

### 6.1 Customer Origin Analysis (Sri Lankan Market Context)

**Geographic distribution with regional insights:**

```python
# Top countries analysis
country_analysis = df.groupby('country').agg({
    'is_canceled': ['count', 'sum', 'mean'],
    'adr': 'mean',
    'lead_time': 'mean'
}).round(2)

country_analysis.columns = ['Bookings', 'Cancellations', 'Cancel_Rate', 'Avg_ADR', 'Avg_Lead_Time']
country_analysis = country_analysis[country_analysis['Bookings'] >= 100].sort_values('Bookings', ascending=False)

print("üåç TOP SOURCE MARKETS:")
print(country_analysis.head(15))

# Sri Lankan market focus - if available in data
if 'LKA' in df['country'].values:
    lka_analysis = df[df['country'] == 'LKA']
    print("\nüá±üá∞ SRI LANKAN MARKET INSIGHTS:")
    print(f"Total bookings: {len(lka_analysis)}")
    print(f"Cancellation rate: {lka_analysis['is_canceled'].mean():.3f}")
    print(f"Average ADR: ${lka_analysis['adr'].mean():.2f}")
    print(f"Average lead time: {lka_analysis['lead_time'].mean():.1f} days")
```

### 6.2 Market Segment Analysis

**Business intelligence for market positioning:**

```python
# Market segment deep dive
segment_analysis = df.groupby(['market_segment', 'is_canceled']).agg({
    'adr': ['mean', 'count'],
    'lead_time': 'mean',
    'total_of_special_requests': 'mean'
}).round(2)

# Revenue per segment calculation
segment_revenue = df.groupby('market_segment').apply(
    lambda x: (x['adr'] * (x['stays_in_weekend_nights'] + x['stays_in_week_nights']) * (1 - x['is_canceled'])).sum()
).sort_values(ascending=False)

print("üíº MARKET SEGMENT PERFORMANCE:")
print("Revenue by segment (total realized revenue):")
print(segment_revenue)

# Visualization
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Cancellation rate by segment
segment_cancel_rate = df.groupby('market_segment')['is_canceled'].mean().sort_values()
segment_cancel_rate.plot(kind='barh', ax=axes[0,0])
axes[0,0].set_title('Cancellation Rate by Market Segment')

# ADR by segment
sns.boxplot(data=df, y='market_segment', x='adr', ax=axes[0,1])
axes[0,1].set_title('ADR Distribution by Market Segment')

# Lead time by segment
sns.boxplot(data=df, y='market_segment', x='lead_time', ax=axes[1,0])
axes[1,0].set_title('Lead Time by Market Segment')

# Revenue contribution
segment_revenue.plot(kind='pie', ax=axes[1,1], autopct='%1.1f%%')
axes[1,1].set_title('Revenue Contribution by Segment')

plt.tight_layout()
plt.show()
```

### 6.2 Data Pipeline Optimization Analysis

**Strategic analysis of column dropping pipeline for model optimization:**

```python
def analyze_column_dropping_pipeline(df):
    """
    Systematic analysis for optimal feature selection and column dropping strategy
    """
    print("üóÇÔ∏è COLUMN DROPPING PIPELINE ANALYSIS:")
    
    # Define dropping categories and rationale
    dropping_analysis = {
        'high_missing_columns': {
            'columns': ['agent', 'company'],
            'rationale': 'Excessive missing values (>70%)',
            'impact_type': 'data_quality'
        },
        'high_cardinality_columns': {
            'columns': ['country'],
            'rationale': 'High cardinality (150+ unique values) difficult to encode',
            'impact_type': 'encoding_complexity'
        },
        'data_leakage_columns': {
            'columns': ['reservation_status', 'reservation_status_date'],
            'rationale': 'Direct information about cancellation outcome',
            'impact_type': 'data_leakage'
        },
        'low_predictive_columns': {
            'columns': ['booking_changes'],
            'rationale': 'Low correlation and potential temporal dependency',
            'impact_type': 'predictive_power'
        }
    }
    
    # Analyze each category
    total_columns_before = len(df.columns)
    columns_to_drop = []
    
    for category, info in dropping_analysis.items():
        print(f"\nüìã {category.upper().replace('_', ' ')}:")
        print(f"Rationale: {info['rationale']}")
        
        for col in info['columns']:
            if col in df.columns:
                # Basic statistics
                missing_pct = df[col].isnull().sum() / len(df) * 100
                unique_vals = df[col].nunique()
                
                # Correlation with target if numeric
                if df[col].dtype in ['int64', 'float64'] and col != 'is_canceled':
                    correlation = df[col].corr(df['is_canceled'])
                    print(f"  ‚Ä¢ {col}: {missing_pct:.1f}% missing, {unique_vals} unique values, correlation: {correlation:.3f}")
                else:
                    print(f"  ‚Ä¢ {col}: {missing_pct:.1f}% missing, {unique_vals} unique values")
                
                # Specific analysis based on column
                if col == 'agent':
                    # Analyze agent impact
                    agent_present = df[df[col].notna()]
                    agent_missing = df[df[col].isna()]
                    if len(agent_present) > 0 and len(agent_missing) > 0:
                        cancel_rate_present = agent_present['is_canceled'].mean()
                        cancel_rate_missing = agent_missing['is_canceled'].mean()
                        print(f"    Cancellation rate with agent: {cancel_rate_present:.3f}")
                        print(f"    Cancellation rate without agent: {cancel_rate_missing:.3f}")
                
                elif col == 'company':
                    # Analyze company impact
                    company_bookings = df[df[col].notna()]
                    if len(company_bookings) > 0:
                        company_cancel_rate = company_bookings['is_canceled'].mean()
                        individual_cancel_rate = df[df[col].isna()]['is_canceled'].mean()
                        print(f"    Company booking cancellation rate: {company_cancel_rate:.3f}")
                        print(f"    Individual booking cancellation rate: {individual_cancel_rate:.3f}")
                
                elif col == 'country':
                    # Analyze country diversity
                    top_countries = df[col].value_counts().head(10)
                    print(f"    Top 10 countries represent {top_countries.sum() / len(df) * 100:.1f}% of bookings")
                    
                    # Calculate cancellation rates for top countries
                    country_cancel_rates = df.groupby(col)['is_canceled'].mean().sort_values(ascending=False)
                    print(f"    Country cancel rate range: {country_cancel_rates.min():.3f} - {country_cancel_rates.max():.3f}")
                
                elif col in ['reservation_status', 'reservation_status_date']:
                    # Analyze data leakage risk
                    if col == 'reservation_status':
                        status_counts = df[col].value_counts()
                        print(f"    Status distribution: {dict(status_counts)}")
                        
                        # Check direct correlation with cancellation
                        if 'Canceled' in df[col].values:
                            canceled_status_match = (df[df['is_canceled'] == 1][col] == 'Canceled').mean()
                            print(f"    Direct leakage: {canceled_status_match:.1%} of canceled bookings have 'Canceled' status")
                
                elif col == 'booking_changes':
                    # Analyze booking changes impact
                    changes_correlation = df[col].corr(df['is_canceled'])
                    changes_mean_by_cancel = df.groupby('is_canceled')[col].mean()
                    print(f"    Average changes (not canceled): {changes_mean_by_cancel[0]:.2f}")
                    print(f"    Average changes (canceled): {changes_mean_by_cancel[1]:.2f}")
                
                columns_to_drop.append(col)
        
        print(f"  üìä Category impact: {info['impact_type']}")
    
    # Overall pipeline impact analysis
    print(f"\nüéØ PIPELINE OPTIMIZATION SUMMARY:")
    print(f"Original columns: {total_columns_before}")
    print(f"Columns to drop: {len(columns_to_drop)}")
    print(f"Remaining columns: {total_columns_before - len(columns_to_drop)}")
    print(f"Reduction percentage: {len(columns_to_drop) / total_columns_before * 100:.1f}%")
    
    # Create a clean dataset for comparison
    df_cleaned = df.drop(columns=columns_to_drop, errors='ignore')
    
    # Memory and processing benefits
    memory_before = df.memory_usage(deep=True).sum() / 1024**2  # MB
    memory_after = df_cleaned.memory_usage(deep=True).sum() / 1024**2  # MB
    memory_reduction = (memory_before - memory_after) / memory_before * 100
    
    print(f"\nüíæ PERFORMANCE BENEFITS:")
    print(f"Memory usage reduction: {memory_reduction:.1f}%")
    print(f"Encoding complexity reduction: Eliminated high-cardinality country column")
    print(f"Data quality improvement: Removed high-missing columns")
    print(f"Model reliability: Eliminated data leakage sources")
    
    # Visualization of dropping impact
    plt.figure(figsize=(15, 10))
    
    # Column categories pie chart
    plt.subplot(2, 3, 1)
    category_counts = {cat.replace('_columns', '').replace('_', ' ').title(): len(info['columns']) 
                      for cat, info in dropping_analysis.items()}
    plt.pie(category_counts.values(), labels=category_counts.keys(), autopct='%1.0f%%')
    plt.title('Columns Dropped by Category')
    
    # Missing value comparison
    plt.subplot(2, 3, 2)
    original_missing = df.isnull().sum().sum()
    cleaned_missing = df_cleaned.isnull().sum().sum()
    plt.bar(['Original', 'After Dropping'], [original_missing, cleaned_missing])
    plt.title('Total Missing Values')
    plt.ylabel('Count')
    
    # Memory usage comparison
    plt.subplot(2, 3, 3)
    plt.bar(['Original', 'After Dropping'], [memory_before, memory_after])
    plt.title('Memory Usage (MB)')
    plt.ylabel('Memory (MB)')
    
    # Feature count comparison
    plt.subplot(2, 3, 4)
    feature_counts = {
        'Numerical': len(df_cleaned.select_dtypes(include=[np.number]).columns),
        'Categorical': len(df_cleaned.select_dtypes(exclude=[np.number]).columns)
    }
    plt.pie(feature_counts.values(), labels=feature_counts.keys(), autopct='%1.0f%%')
    plt.title('Feature Types After Cleaning')
    
    # Cancellation rate stability check
    plt.subplot(2, 3, 5)
    original_cancel_rate = df['is_canceled'].mean()
    cleaned_cancel_rate = df_cleaned['is_canceled'].mean()
    plt.bar(['Original', 'After Dropping'], [original_cancel_rate, cleaned_cancel_rate])
    plt.title('Cancellation Rate Stability')
    plt.ylabel('Cancellation Rate')
    
    # Top remaining features by correlation
    plt.subplot(2, 3, 6)
    if 'is_canceled' in df_cleaned.columns:
        remaining_correlations = df_cleaned.corr()['is_canceled'].abs().drop('is_canceled').sort_values(ascending=False).head(8)
        remaining_correlations.plot(kind='barh')
        plt.title('Top Correlations After Cleaning')
        plt.xlabel('|Correlation with Cancellation|')
    
    plt.tight_layout()
    plt.show()
    
    return df_cleaned, columns_to_drop, dropping_analysis

# Perform column dropping pipeline analysis
df_optimized, dropped_columns, pipeline_analysis = analyze_column_dropping_pipeline(df)

print(f"\n‚úÖ PIPELINE EXECUTION COMPLETE:")
print(f"Dropped columns: {dropped_columns}")
print(f"Optimized dataset shape: {df_optimized.shape}")
```

---

## üîß Phase 7: Feature Engineering Opportunities

### 7.1 Create New Predictive Features

**Advanced feature creation for enhanced prediction:**

```python
# Feature engineering based on domain knowledge
df_enhanced = df.copy()

# 1. Total stay duration
df_enhanced['total_stay_nights'] = df['stays_in_weekend_nights'] + df['stays_in_week_nights']

# 2. Guest composition features
df_enhanced['total_guests'] = df['adults'] + df['children'] + df['babies']
df_enhanced['is_family'] = ((df['children'] > 0) | (df['babies'] > 0)).astype(int)
df_enhanced['adults_to_children_ratio'] = df['adults'] / (df['children'] + df['babies'] + 1)

# 3. Booking behavior features
df_enhanced['is_repeated_guest_binary'] = df['is_repeated_guest'].astype(int)
df_enhanced['has_previous_cancellations'] = (df['previous_cancellations'] > 0).astype(int)
df_enhanced['booking_changes_binary'] = (df['booking_changes'] > 0).astype(int)

# 4. Revenue and pricing features
df_enhanced['revenue_per_night'] = df['adr'] * df_enhanced['total_guests']
df_enhanced['total_revenue'] = df_enhanced['revenue_per_night'] * df_enhanced['total_stay_nights']

# 5. Time-based features
df_enhanced['booking_quarter'] = pd.to_datetime(df_enhanced['reservation_status_date']).dt.quarter
df_enhanced['is_weekend_arrival'] = df_enhanced['arrival_date_week_number'].apply(lambda x: 1 if x % 7 in [5, 6] else 0)

# 6. Risk indicators
df_enhanced['high_lead_time'] = (df['lead_time'] > df['lead_time'].quantile(0.75)).astype(int)
df_enhanced['special_requests_score'] = df['total_of_special_requests'] / df_enhanced['total_guests']

# Analyze new features
new_features = ['total_stay_nights', 'total_guests', 'is_family', 'revenue_per_night', 
                'has_previous_cancellations', 'high_lead_time', 'special_requests_score']

print("üõ†Ô∏è ENGINEERED FEATURES ANALYSIS:")
for feature in new_features:
    correlation_with_cancel = df_enhanced[feature].corr(df_enhanced['is_canceled'])
    print(f"{feature}: Correlation with cancellation = {correlation_with_cancel:.3f}")
```

### 7.2 Feature Importance Analysis

**Statistical significance testing:**

```python
from scipy.stats import chi2_contingency, ttest_ind

# Chi-square tests for categorical variables
categorical_vars = ['hotel', 'meal', 'market_segment', 'distribution_channel', 
                   'deposit_type', 'customer_type', 'is_family']

print("üìä STATISTICAL SIGNIFICANCE TESTS:")
print("\nChi-square tests for categorical variables:")
for var in categorical_vars:
    if var in df_enhanced.columns:
        contingency_table = pd.crosstab(df_enhanced[var], df_enhanced['is_canceled'])
        chi2, p_value, _, _ = chi2_contingency(contingency_table)
        print(f"{var}: Chi2 = {chi2:.3f}, p-value = {p_value:.6f}")

# T-tests for numerical variables
numerical_vars = ['lead_time', 'adr', 'total_stay_nights', 'total_guests', 'special_requests_score']

print("\nT-tests for numerical variables:")
for var in numerical_vars:
    if var in df_enhanced.columns:
        canceled = df_enhanced[df_enhanced['is_canceled'] == 1][var]
        not_canceled = df_enhanced[df_enhanced['is_canceled'] == 0][var]
        t_stat, p_value = ttest_ind(canceled, not_canceled)
        print(f"{var}: t-statistic = {t_stat:.3f}, p-value = {p_value:.6f}")
```

---

## üìà Phase 8: Advanced Analytics

### 8.1 Customer Segmentation Analysis

**RFM-style analysis adapted for hotel bookings:**

```python
# Customer behavior segmentation
customer_segments = df.groupby(['customer_type', 'market_segment']).agg({
    'is_canceled': 'mean',
    'adr': 'mean',
    'lead_time': 'mean',
    'total_of_special_requests': 'mean',
    'booking_changes': 'mean'
}).round(3)

print("üéØ CUSTOMER SEGMENTATION INSIGHTS:")
print(customer_segments)

# Risk scoring system
def calculate_risk_score(row):
    score = 0
    
    # Lead time risk (longer lead time = higher risk)
    if row['lead_time'] > 120:
        score += 3
    elif row['lead_time'] > 60:
        score += 2
    elif row['lead_time'] > 30:
        score += 1
    
    # Previous cancellation history
    if row['previous_cancellations'] > 0:
        score += 4
    
    # Market segment risk
    high_risk_segments = ['Online TA', 'Offline TA/TO']
    if row['market_segment'] in high_risk_segments:
        score += 2
    
    # Deposit type (no deposit = higher risk)
    if row['deposit_type'] == 'No Deposit':
        score += 2
    
    # Booking changes (indicates uncertainty)
    if row['booking_changes'] > 0:
        score += 1
    
    return score

df_enhanced['risk_score'] = df.apply(calculate_risk_score, axis=1)

# Analyze risk score effectiveness
risk_analysis = df_enhanced.groupby('risk_score').agg({
    'is_canceled': ['count', 'mean'],
    'adr': 'mean'
}).round(3)

print("\nüö® RISK SCORE ANALYSIS:")
print(risk_analysis)
```

### 8.2 Revenue Impact Assessment

**Financial implications of cancellations:**

```python
# Revenue loss analysis
total_bookings = len(df)
total_cancellations = df['is_canceled'].sum()
total_revenue_loss = df[df['is_canceled'] == 1]['adr'].sum()
avg_revenue_per_canceled_booking = df[df['is_canceled'] == 1]['adr'].mean()

print("üí∞ REVENUE IMPACT ANALYSIS:")
print(f"Total bookings: {total_bookings:,}")
print(f"Total cancellations: {total_cancellations:,}")
print(f"Cancellation rate: {total_cancellations/total_bookings:.1%}")
print(f"Revenue lost to cancellations: ${total_revenue_loss:,.2f}")
print(f"Average revenue per canceled booking: ${avg_revenue_per_canceled_booking:.2f}")

# Revenue opportunities by reducing cancellations
potential_revenue_recovery = []
for reduction_pct in [0.1, 0.2, 0.3, 0.4, 0.5]:
    recovered_revenue = total_revenue_loss * reduction_pct
    potential_revenue_recovery.append({
        'Reduction_Percentage': f"{reduction_pct:.0%}",
        'Revenue_Recovered': f"${recovered_revenue:,.2f}",
        'Bookings_Saved': f"{total_cancellations * reduction_pct:.0f}"
    })

recovery_df = pd.DataFrame(potential_revenue_recovery)
print("\nüìä REVENUE RECOVERY POTENTIAL:")
print(recovery_df)
```

---

## üìù Phase 9: Business Insights & Recommendations

### 9.1 Key Findings Documentation

**Systematic insight generation:**

```python
# Generate automated insights
insights = []

# 1. Lead time insights
avg_lead_time_canceled = df[df['is_canceled'] == 1]['lead_time'].mean()
avg_lead_time_confirmed = df[df['is_canceled'] == 0]['lead_time'].mean()
insights.append(f"Canceled bookings have {avg_lead_time_canceled/avg_lead_time_confirmed:.1f}x longer lead time on average")

# 2. Price sensitivity insights
avg_adr_canceled = df[df['is_canceled'] == 1]['adr'].mean()
avg_adr_confirmed = df[df['is_canceled'] == 0]['adr'].mean()
if avg_adr_canceled > avg_adr_confirmed:
    insights.append(f"Higher priced bookings (${avg_adr_canceled:.2f}) have higher cancellation rates than lower priced (${avg_adr_confirmed:.2f})")
else:
    insights.append(f"Lower priced bookings show higher cancellation rates")

# 3. Seasonal insights
monthly_cancel_rates = df.groupby('arrival_date_month')['is_canceled'].mean()
peak_cancel_month = monthly_cancel_rates.idxmax()
low_cancel_month = monthly_cancel_rates.idxmin()
insights.append(f"Highest cancellation rates occur in {peak_cancel_month} ({monthly_cancel_rates[peak_cancel_month]:.1%})")
insights.append(f"Lowest cancellation rates occur in {low_cancel_month} ({monthly_cancel_rates[low_cancel_month]:.1%})")

# 4. Market segment insights
segment_cancel_rates = df.groupby('market_segment')['is_canceled'].mean().sort_values(ascending=False)
highest_risk_segment = segment_cancel_rates.index[0]
lowest_risk_segment = segment_cancel_rates.index[-1]
insights.append(f"Highest risk market segment: {highest_risk_segment} ({segment_cancel_rates.iloc[0]:.1%} cancellation rate)")
insights.append(f"Most reliable market segment: {lowest_risk_segment} ({segment_cancel_rates.iloc[-1]:.1%} cancellation rate)")

print("üîç KEY BUSINESS INSIGHTS:")
for i, insight in enumerate(insights, 1):
    print(f"{i}. {insight}")
```

### 9.2 Sri Lankan Tourism Market Recommendations

**Context-specific recommendations:**

```python
print("\nüá±üá∞ SRI LANKAN HOSPITALITY MARKET RECOMMENDATIONS:")
print("""
1. **Seasonal Strategy Optimization**:
   - Focus marketing efforts during low-cancellation months
   - Implement dynamic pricing during high-risk periods
   - Develop monsoon-season packages to maintain bookings

2. **Lead Time Management**:
   - Implement progressive payment schedules for long lead time bookings
   - Create incentives for shorter booking windows
   - Develop flexible cancellation policies for advance bookings

3. **Market Segment Targeting**:
   - Prioritize low-risk segments for premium inventory
   - Develop segment-specific retention strategies
   - Create corporate partnership programs for reliable segments

4. **Revenue Protection Strategies**:
   - Implement risk-based deposit requirements
   - Develop overbooking algorithms based on cancellation predictions
   - Create last-minute booking incentives

5. **Cultural Tourism Integration**:
   - Package cultural experiences to increase booking commitment
   - Partner with local tour operators for comprehensive offerings
   - Develop narrative-driven itineraries (future Serendipity platform integration)
""")
```

---

## üéì Phase 10: Academic Documentation

### 10.1 EDA Summary Report

**Academic-standard documentation:**

```python
# Generate comprehensive EDA summary
eda_summary = {
    'Dataset Overview': {
        'Total Records': len(df),
        'Features': len(df.columns),
        'Target Variable': 'is_canceled',
        'Cancellation Rate': f"{df['is_canceled'].mean():.1%}",
        'Date Range': f"{df['arrival_date_year'].min()} - {df['arrival_date_year'].max()}"
    },
    
    'Data Quality': {
        'Missing Values': df.isnull().sum().sum(),
        'Duplicate Records': df.duplicated().sum(),
        'Logical Inconsistencies': len(df[(df['adults'] == 0) & (df['children'] == 0) & (df['babies'] == 0)])
    },
    
    'Key Correlations': dict(df.corr()['is_canceled'].abs().nlargest(5)),
    
    'Business Impact': {
        'Revenue at Risk': f"${df[df['is_canceled'] == 1]['adr'].sum():,.2f}",
        'Average Loss per Cancellation': f"${df[df['is_canceled'] == 1]['adr'].mean():.2f}",
        'High Risk Segment': df.groupby('market_segment')['is_canceled'].mean().idxmax()
    }
}

print("üìä EDA EXECUTIVE SUMMARY:")
for section, metrics in eda_summary.items():
    print(f"\n{section}:")
    for metric, value in metrics.items():
        print(f"  ‚Ä¢ {metric}: {value}")
```

### 10.2 Methodology Documentation

**Research methodology for academic compliance:**

```python
print("""
üìö EDA METHODOLOGY SUMMARY:

**Statistical Methods Applied:**
1. Descriptive Statistics: Mean, median, mode, standard deviation, skewness, kurtosis
2. Inferential Statistics: Chi-square tests, t-tests for significance testing
3. Correlation Analysis: Pearson correlation for continuous variables
4. Distribution Analysis: Normality testing using Q-Q plots and Shapiro-Wilk tests

**Visualization Techniques:**
1. Univariate: Histograms, box plots, violin plots, density plots
2. Bivariate: Scatter plots, correlation heatmaps, grouped bar charts
3. Multivariate: Pair plots, parallel coordinates, 3D scatter plots

**Business Intelligence Methods:**
1. Cohort Analysis: Temporal patterns in booking behavior
2. RFM Adaptation: Recency, frequency, monetary analysis for hotel context
3. Geographic Analysis: Source market performance evaluation
4. Risk Scoring: Multi-factor risk assessment model

**Feature Engineering Approach:**
1. Domain Knowledge Integration: Hotel industry best practices
2. Statistical Feature Selection: Correlation and significance testing
3. Business Logic Features: Revenue calculation, stay duration metrics
4. Behavioral Indicators: Repeat guest patterns, special request analysis

**Quality Assurance:**
1. Data Validation: Logical consistency checks
2. Outlier Analysis: IQR method and business rule validation
3. Missing Data Strategy: Pattern analysis and imputation recommendations
4. Bias Detection: Temporal and segment-based bias assessment
""")
```

---

## ‚úÖ Phase 11: Action Items & Next Steps

### 11.1 EDA Completion Checklist

```python
eda_checklist = [
    "‚úÖ Dataset overview and structure analysis completed",
    "‚úÖ Data quality assessment and missing value analysis",
    "‚úÖ Reservation status and date consistency validation",
    "‚úÖ High missing value column assessment (agent/company analysis)",
    "‚úÖ Lead time impact analysis with statistical testing",
    "‚úÖ Comprehensive univariate analysis for all features",
    "‚úÖ Bivariate analysis focusing on cancellation relationships", 
    "‚úÖ Mean encoding analysis for categorical variables",
    "‚úÖ Geographic and market segment deep dive",
    "‚úÖ Column dropping pipeline optimization analysis",
    "‚úÖ Hotel-specific performance comparison",
    "‚úÖ Feature engineering and new variable creation",
    "‚úÖ Statistical significance testing",
    "‚úÖ Revenue impact and business intelligence analysis",
    "‚úÖ Risk scoring and customer segmentation",
    "‚úÖ Academic documentation and methodology summary",
    "‚úÖ Sri Lankan market context integration",
    "‚ñ° Prepare data for model training pipeline",
    "‚ñ° Document feature selection rationale",
    "‚ñ° Create data preprocessing recommendations"
]

print("üìã EDA COMPLETION STATUS:")
for item in eda_checklist:
    print(item)
```

### 11.2 Model Development Preparation

**Prepare insights for model building:**

```python
print("""
üöÄ NEXT STEPS FOR MODEL DEVELOPMENT:

**Feature Selection Recommendations:**
1. High-impact numerical features: lead_time, adr, total_stay_nights
2. Critical categorical features: market_segment, deposit_type, customer_type
3. Engineered features: risk_score, is_family, total_guests
4. Remove low-impact features: country (high cardinality), company (sparse)

**Preprocessing Pipeline:**
1. Handle missing values using domain-appropriate methods
2. Encode categorical variables using target encoding for high cardinality
3. Scale numerical features using StandardScaler
4. Apply log transformation to skewed distributions

**Model Strategy:**
1. Start with baseline models: LogisticRegression, RandomForest
2. Progress to ensemble methods: XGBoost, LightGBM
3. Implement neural networks: PyTorch MLP for comparison
4. Use cross-validation with stratification for class imbalance

**Evaluation Framework:**
1. Primary metric: F1-Score (balanced precision-recall)
2. Secondary metrics: ROC-AUC, Precision, Recall
3. Business metric: Revenue impact simulation
4. Interpretability: SHAP values for model explanation

**Academic Requirements:**
1. Document hyperparameter optimization process
2. Include statistical significance testing of model differences
3. Provide comprehensive model interpretation
4. Address ethical considerations in hospitality AI
""")
```

---

## üìö References and Further Reading

## üìù Recent EDA Enhancements Summary

**The following analyses have been added based on specific research requirements:**

### ‚úÖ New Analytical Components:

1. **Reservation Status Consistency Validation (Section 2.2)**
   - Cross-tabulation of reservation_status vs is_canceled
   - Data integrity checking for canceled bookings
   - Temporal analysis of reservation_status_date patterns
   - Business logic validation for booking lifecycle

2. **High Missing Value Column Assessment (Section 2.3)**
   - Systematic analysis of agent and company columns (>70% missing)
   - Chi-square testing for missingness patterns vs cancellations
   - Business impact assessment for column removal decisions
   - Recommendations for strategic feature elimination

3. **Lead Time Impact Analysis (Section 2.5)**
   - Statistical significance testing (Mann-Whitney U test)
   - Lead time binning and cancellation rate analysis
   - Correlation analysis with visualization
   - Monthly seasonal patterns in lead time behavior
   - Percentile comparison between canceled/not-canceled groups

4. **Mean Encoding Analysis for Categorical Variables (Section 4.3)**
   - Effectiveness ratio calculation for each categorical feature
   - F-statistic testing for category significance
   - Cancellation rate range analysis per category
   - Priority ranking for encoding strategies
   - Visual error bar plots with statistical validation

5. **Column Dropping Pipeline Optimization (Section 6.2)**
   - Systematic categorization of columns by removal rationale
   - Memory usage and performance impact analysis
   - Data leakage risk assessment
   - Feature complexity reduction strategies
   - Visual pipeline optimization dashboard

### üéØ Integration with Existing Framework:

These enhancements complement the existing 968-line EDA structure while maintaining:
- Academic rigor with statistical testing
- Business context for hospitality industry
- Sri Lankan tourism market focus
- NIB 7072 coursework alignment
- Comprehensive visualization strategies

### üìä EDA Coverage Status:

**From Original 18 Requirements:**
- ‚úÖ Target distribution analysis (existing)
- ‚úÖ Missing value patterns with missingno (existing + enhanced)
- ‚úÖ Reservation status validation (NEW)
- ‚úÖ Agent/company column assessment (NEW)
- ‚úÖ Lead time cancellation analysis (NEW + enhanced existing)
- ‚úÖ Spatial/geographic analysis (existing)
- ‚úÖ Correlation analysis (existing)
- ‚úÖ Mean encoding evaluation (NEW)
- ‚úÖ Outlier detection and handling (existing)
- ‚úÖ Column dropping pipeline (NEW)

**Modeling-Specific Items (belong in separate modeling.md):**
- SMOTE class balancing (preprocessing phase)
- Hyperparameter tuning (model training phase) 
- Cross-validation (model evaluation phase)
- Model comparison (evaluation phase)

---

## üìö References and Further Reading

**Academic and Industry Resources:**

```python
print("""
üìñ RECOMMENDED REFERENCES:

**Academic Literature:**
1. Chen, C. et al. (2019). "Hotel booking demand forecasting using machine learning"
2. Morales, D. & Wang, J. (2010). "Revenue management in hospitality industry"
3. Kumar, A. et al. (2021). "Predictive analytics in tourism: A systematic review"

**Industry Reports:**
1. STR Global Hotel Performance Reports
2. Sri Lanka Tourism Development Authority Statistics
3. Hospitality Industry Revenue Management Best Practices

**Technical Resources:**
1. Scikit-learn Documentation for ML Pipeline
2. MLflow Documentation for Experiment Tracking
3. SHAP Documentation for Model Interpretability
4. Plotly Documentation for Interactive Visualizations

**Business Context:**
1. Sri Lankan Tourism Strategic Plan 2022-2027
2. South Asian Hospitality Market Analysis
3. Cultural Tourism Development Framework
""")
```

---

## üéØ Conclusion

This comprehensive EDA framework provides a systematic approach to understanding hotel booking cancellation patterns with specific focus on academic rigor and Sri Lankan tourism market applications. Follow each phase sequentially, document findings thoroughly, and maintain statistical integrity throughout the analysis.

**Key Success Metrics:**
- Complete understanding of data structure and quality
- Identification of top predictive features
- Business-relevant insights for hospitality strategy
- Statistical validation of findings
- Academic-standard documentation

**Final Note:** This EDA serves as the foundation for the NIB 7072 coursework and future development of the "Serendipity by Design" platform. Ensure all analyses align with both academic requirements and practical business applications in Sri Lankan hospitality sector.